# PRD 0010E: Bare Layout & Input (vertical stream, pinned footer/send, auto-scroll JS) + SapAgent Wiring

**Overview**: Create a minimal chat UI page at /admin/sap_collaborate for SAP interactions, featuring a vertical message stream, bottom-pinned input form, auto-scrolling JS, and direct wiring to SapAgent for submitting questions and streaming answers. This establishes a bare-metal base for oversight, enabling users to ask questions (e.g., "Generate PRD for Plaid holdings sync") and receive streamed responses without sidebars or advanced controls.

## Requirements

### 1) Bare Layout & Stream
Use ERB view (admin/sap_collaborate/index.html.erb) with Tailwind CSS for a full-height vertical container (#chat-stream flex-col-reverse for natural bottom-align). Render messages as simple div bubbles (user: right-aligned blue, assistant: left-aligned gray) via partial (_message.html.erb). Stub future gear menu as empty div.

### 2) Pinned Input & Send
Footer form with textarea (pinned via fixed/absolute positioning, min-height 100px) and submit button ("Send"). On submit, POST to /admin/sap_collaborate/ask → clear input, append user message to stream.

### 3) Auto-Scroll JS
Stimulus controller (chat_controller.js) with connected() for initial scrollToBottom (#chat-stream.scrollTop = scrollHeight); MutationObserver on #chat-stream for dynamic scroll on new messages. Handle mobile keyboard overlap with resize event listener (adjust padding-bottom via getComputedStyle).

### 4) Turbo Streams/Polling Wiring
Prefer Turbo Streams for real-time appends (broadcast_to "sap_channel" with partial chunks). Fallback to JS polling (setInterval 3s with exponential backoff: 3s→5s→10s max 3 retries) on /admin/sap_collaborate/status → append JSON chunks to stream if new. Use Sidekiq job (SapAgentJob) for chunking: Enqueue on submit, process SapAgent.generate in background, broadcast each chunk/error as Turbo Stream append.

### 5) SapAgent Connection
Controller action ask (POST) → validate input, enqueue SapAgentJob.perform_later(user_id, prompt), return Turbo Stream to append "Thinking..." placeholder. SapAgentJob: Call AiFinancialAdvisor.ask(prompt) via local Ollama wrapper (HTTP to localhost:11434), yield chunks (e.g., stream: true in Ollama params), broadcast each as _chunk.html.erb (markdown-rendered text). On error, broadcast red bubble with ID/correlation.

### 6) Non-Functional
Page loads in <2s; stream chunks every 1-2s; support 10k+ token responses without lag. Admin-only (before_action :authenticate_user!, :authorize_admin).

### 7) alexrudall Gist Direction
Adapt GetAiResponse job structure: Enqueue SapAgentJob.perform_later(sap_run_id) instead of chat_id; use call_openai equivalent for Ollama (AiFinancialAdvisor.chat with stream: proc); create_messages to pre-build assistant message stubs; stream_proc to update message content with new chunks and broadcast updates. Use Message model pattern for SapMessage (new model: belongs_to :sap_run, enum role, after_create/update_commit broadcasts). Broadcast append/updated as in gist (broadcast_append_later_to with partial, target #chat-stream). Ignore OpenAI-specific params; map to Ollama (model: "llama3.1:70b", prompt, stream: true). For multi-response (n:1), simplify to single stream.

## Architectural Context
Aligns with Rails MVC: SapCollaborateController for actions (ask, status); new SapRun model (tracks session with correlation_id) and SapMessage (for chunks). No migrations beyond basics (add_table :sap_runs/:sap_messages). Use plaid-ruby patterns for services (SapAgentService for Ollama calls, wrapping HTTP with JSON blobs from FinancialSnapshotJob). Inject vision.md/personas into SapAgent prompts for context. Local-only: Ollama on-premises, no cloud. Defer vector DB—use static MD files in RAG. UI: Tailwind/DaisyUI for responsive (mobile-first with flex); ViewComponent optional for _message if complexity grows.

## Acceptance Criteria
- Load /admin/sap_collaborate → empty stream, pinned input visible at bottom.
- Type question → submit → user bubble appends right-aligned, "Thinking..." left-aligned.
- SapAgent processes → chunks stream in (e.g., partial sentences build up), auto-scrolls to bottom.
- Full response completes → final bubble markdown-rendered (e.g., bold/lists).
- Error (e.g., Ollama down) → red bubble with "Error: [msg] (ID: xyz)".
- Polling fallback: If Turbo fails, JS polls and appends chunks without reload.
- Mobile: Input doesn't overlap stream on keyboard open (dynamic padding).
- Admin-gate: Non-admins 403 on access.

## Test Cases
- **Unit**: MiniTest for SapAgentService.ask (mock HTTP to Ollama/SmartProxy, assert chunk yields).
- **Integration**: MiniTest with VCR cassettes for end-to-end: Record real SmartProxy interactions (e.g., cassette for "Test question" → chunk responses); assert stream appends in response body. Use WebMock for stubs in non-VCR tests. Edge: Empty prompt → validation error bubble; long response (>5k tokens) → no truncation.

## Workflow
Junie: Pull from main, git checkout -b feature/agent-4.5-bare-chat-sap. Read <project root>/knowledge_base/prds/prds-junie-log/junie-log-requirement.md for logging. Ask questions (e.g., "Clarify polling backoff max?") and build a plan in junie-log.md before coding. Use Claude Sonnet 4.5 default in RubyMine. Implement, test with MiniTest/VCR using real SmartProxy (cassettes capture live runs), commit only if green (no errors in logs). Push branch for review.

## Next Steps
Assign to Junie now? Revise gist adaptations (e.g., skip multi-response n>1)?
