### PRD 0040: Self-Debug Loop
**Overview**: Implement a self-debug loop for CWA to run tests, parse logs, identify failures, and retry fixes up to 2x before escalating. This embodies Junie pillars (checkpoints/failures) by enabling autonomous error handling in code gen, tying to vision for reliable AI-driven development in the family-office platform.

**Requirements**:
1) **Debug Loop Logic**: In CWA agent, after impl (tool calls from 0060C), trigger loop: Run MiniTest/VCR + rubocop/brakeman via SafeShellTool; parse output logs for failures (regex for "F", "Error", line numbers); generate fix prompts based on errors (e.g., "Fix syntax error at line 42"); retry tool calls with fixes; max 2 retries.
2) **Checkpoints & Failures**: Use context hash checkpoints (snapshot code/state pre/post-debug); on failure, populate failure analysis section in log template (0060B); escalate to Coordinator if unresolved (route_to with "ball_with=Human", state=blocked).
3) **Integration with Gem**: Hook into ai-agents after_tool callback for debug initiation; update context (feedback_history: debug entries); JSON outputs for parsed logs (e.g., {errors: [{type, line, msg}]}).
4) **Guardrails**: Limit retries=2; timeout 60s per run; whitelist test cmds (rake test, rubocop -a); dry-run v1 (simulate test outputs); graceful failure: Log partial results, escalate without crash.
5) **Non-Functional**: Loop completes <2min; supports 100-test suites; local-only; logs all iterations in 12-section template/events.ndjson.
6) **Rails Guidance**: Update AiCodeWriterService with .debug_loop(method) for orchestration; parse logs via Ruby regex/StringScanner; use existing test mocks (WebMock/VCR); sandbox all runs (tmp/agent_sandbox/); no new models—append to AiWorkflowRun jsonb if used.

**Architectural Context**: Builds on ai-agents CWA registration (tools/callbacks); handoffs update context schema (add debug_history[] to feedback_history). Rails MVC for services; local Ollama/Grok/Claude via SmartProxy; RAG via context (include test suite paths in artifacts). Privacy: No external test runs; RLS on any data access.

**Acceptance Criteria**:
- CWA after impl → auto-runs tests, parses failures, retries fix if error (e.g., syntax → corrected code).
- 2 retries max → escalates on persistent failure, sets state=blocked, ball_with=Human.
- Checkpoints: Pre/post snapshots in logs (e.g., code diff).
- Dry-run: Simulates "test failed" → generates fix prompt without real exec.
- Logs: Debug/retry/checkpoint sections populated with JSON errors.
- Success: Green tests → proceeds to commit staging.
- Failure grace: Unparsable log → minimal escalation log.

**Test Cases**:
- Unit: MiniTest for .debug_loop (mock test output with error, assert retry prompt generated, parses line/type/msg).
- Integration: MiniTest/VCR for CWA handoff with buggy micro-task (cassette prompt; assert retry attempts, final escalation, logs match template). Edge: No errors → no loop; max retries → blocked.

**Workflow**: Junie: Pull from main, git checkout -b feature/prd-60d-self-debug. Read <project root>/knowledge_base/prds/prds-junie-log/junie-log-requirement.md for logging. Ask questions (e.g., "Retry prompt templates?") and build a plan in junie-log.md before coding. Use Claude Sonnet 4.5 default in RubyMine. Implement, test with MiniTest/VCR using real SmartProxy, commit only if green. Push branch for review.

